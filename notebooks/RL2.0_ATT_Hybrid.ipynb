{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Agents for Fraud Detection (Hybrid: Embeddings + Structured Data)\n",
    "\n",
    "This notebook trains RL agents (PPO, A2C, DQN) on a hybrid dataset consisting of:\n",
    "1.  **Embeddings**: Attention-pooled embeddings from a DistilBERT model.\n",
    "2.  **Structured Data**: Original features from the CreditCard dataset (Time, V1-V28, Amount).\n",
    "\n",
    "The combined data goes through a preprocessing pipeline:\n",
    "-   **Scaling**: StandardScaler applied to the concatenated vector.\n",
    "-   **PCA**: Dimensionality reduction on the scaled combined vector.\n",
    "-   **CTGAN**: Data augmentation to balance the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install gymnasium numpy pandas torch stable-baselines3 scikit-learn matplotlib seaborn ctgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f46057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Embeddings\n",
    "print(\"Loading embeddings...\")\n",
    "pkl_data = pd.read_pickle(\"attention_pooled_embeddings.pkl\")\n",
    "embeddings = pkl_data['embeddings']\n",
    "labels = np.array(pkl_data['labels'])\n",
    "uids = pkl_data['uids']\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"UIDs shape: {len(uids)}\")\n",
    "\n",
    "# Load Structured Data\n",
    "print(\"\\nLoading structured data...\")\n",
    "creditcard_df = pd.read_csv(\"ablation_study/creditcard.csv\")\n",
    "\n",
    "# Filter structured data to match the UIDs in the embeddings file\n",
    "# Using uids to select the corresponding rows from the original dataset\n",
    "structured_data = creditcard_df.loc[uids].copy()\n",
    "\n",
    "# Drop Class column to get features\n",
    "structured_features = structured_data.drop('Class', axis=1).values\n",
    "print(f\"Structured features shape: {structured_features.shape}\")\n",
    "\n",
    "# Concatenate Embeddings and Structured Data\n",
    "# We use the labels from the pickle file as the ground truth\n",
    "X = np.hstack([embeddings, structured_features])\n",
    "y = labels\n",
    "\n",
    "print(f\"\\nCombined Data Shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing Pipeline\n",
    "1. Split Train/Test\n",
    "2. Scale (StandardScaler)\n",
    "3. PCA\n",
    "4. CTGAN Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}\")\n",
    "\n",
    "# 2. Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 3. PCA\n",
    "# We want to retain 99% variance, similar to the original notebook\n",
    "pca = PCA(n_components=0.99, whiten=True)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"\\nPCA reduced dimensions from {X_train.shape[1]} to {X_train_pca.shape[1]}\")\n",
    "\n",
    "# 4. CTGAN Augmentation\n",
    "from ctgan import CTGAN\n",
    "\n",
    "# Prepare data for CTGAN (only fraud samples from training set)\n",
    "train_df_pca = pd.DataFrame(X_train_pca, columns=[f'pc{i}' for i in range(X_train_pca.shape[1])])\n",
    "train_df_pca['label'] = y_train\n",
    "\n",
    "fraud_df = train_df_pca[train_df_pca['label'] == 1].drop('label', axis=1)\n",
    "print(f\"Fraud samples for CTGAN: {len(fraud_df)}\")\n",
    "\n",
    "# Train CTGAN\n",
    "ctgan = CTGAN(epochs=200, batch_size=64, pac=1, verbose=True)\n",
    "ctgan.fit(fraud_df)\n",
    "\n",
    "# Generate synthetic samples\n",
    "n_synthetic = len(fraud_df) # Double the fraud samples\n",
    "synthetic_fraud = ctgan.sample(n_synthetic)\n",
    "X_synthetic = synthetic_fraud.values.astype(np.float32)\n",
    "y_synthetic = np.ones(n_synthetic, dtype=np.int64)\n",
    "\n",
    "# Augment Training Set\n",
    "X_train_aug = np.vstack([X_train_pca, X_synthetic])\n",
    "y_train_aug = np.concatenate([y_train, y_synthetic])\n",
    "\n",
    "print(f\"Augmented Train Shape: {X_train_aug.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RL Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDetectionEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment that follows gym interface.\n",
    "    \"\"\"\n",
    "    def __init__(self, features, labels, reward_config=None):\n",
    "        super(FraudDetectionEnv, self).__init__()\n",
    "        \n",
    "        self.features = features.astype(np.float32)\n",
    "        self.labels = labels.astype(np.int64)\n",
    "        self.n_samples = len(features)\n",
    "        self.input_dim = features.shape[1]\n",
    "        \n",
    "        # Define action and observation space\n",
    "        # Action: 0 (Not Fraud), 1 (Fraud)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # Observation: Feature vector\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.input_dim,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        if reward_config is None:\n",
    "            self.reward_config = {\n",
    "                'TP': 10.0,\n",
    "                'FP': -5.0,\n",
    "                'FN': -20.0,\n",
    "                'TN': 1.0\n",
    "            }\n",
    "        else:\n",
    "            self.reward_config = reward_config\n",
    "            \n",
    "        self.current_step = 0\n",
    "        self.indices = np.arange(self.n_samples)\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        np.random.shuffle(self.indices)\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        idx = self.indices[self.current_step]\n",
    "        return self.features[idx]\n",
    "    \n",
    "    def step(self, action):\n",
    "        idx = self.indices[self.current_step]\n",
    "        true_label = self.labels[idx]\n",
    "        \n",
    "        reward = 0\n",
    "        if action == 1 and true_label == 1:\n",
    "            reward = self.reward_config['TP']\n",
    "        elif action == 1 and true_label == 0:\n",
    "            reward = self.reward_config['FP']\n",
    "        elif action == 0 and true_label == 1:\n",
    "            reward = self.reward_config['FN']\n",
    "        elif action == 0 and true_label == 0:\n",
    "            reward = self.reward_config['TN']\n",
    "            \n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.n_samples\n",
    "        truncated = False\n",
    "        \n",
    "        info = {\n",
    "            'true_label': true_label,\n",
    "            'pred_label': action\n",
    "        }\n",
    "        \n",
    "        if not done:\n",
    "            next_obs = self._get_obs()\n",
    "        else:\n",
    "            next_obs = np.zeros(self.input_dim, dtype=np.float32)\n",
    "            \n",
    "        return next_obs, reward, done, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a learning rate schedule\n",
    "def linear_schedule(initial_value):\n",
    "    def schedule(progress_remaining):\n",
    "        return progress_remaining * initial_value\n",
    "    return schedule\n",
    "\n",
    "def train_evaluate_agent(agent_name, X_train, y_train, X_test, y_test, total_timesteps=10000):\n",
    "    print(f\"\\nTraining {agent_name}...\")\n",
    "    \n",
    "    # Create env\n",
    "    env = DummyVecEnv([lambda: FraudDetectionEnv(X_train, y_train)])\n",
    "    \n",
    "    model = None\n",
    "    if agent_name == 'PPO':\n",
    "        # PPO was not in the reference notebook, using default parameters\n",
    "        model = PPO('MlpPolicy', env, verbose=0)\n",
    "    elif agent_name == 'A2C':\n",
    "        # Parameters from RL2.0_ATT.ipynb\n",
    "        model = A2C(\n",
    "            \"MlpPolicy\",\n",
    "            env,\n",
    "            learning_rate=1e-4,\n",
    "            gamma=0.99,\n",
    "            n_steps=5,\n",
    "            ent_coef=0.01,\n",
    "            vf_coef=0.5,\n",
    "            max_grad_norm=0.5,\n",
    "            verbose=0,\n",
    "            device=\"auto\"\n",
    "        )\n",
    "    elif agent_name == 'DQN':\n",
    "        # Parameters from RL2.0_ATT.ipynb\n",
    "        model = DQN(\n",
    "            \"MlpPolicy\",\n",
    "            env,\n",
    "            learning_rate=linear_schedule(1e-4),\n",
    "            buffer_size=100000,\n",
    "            learning_starts=1000,\n",
    "            batch_size=512,\n",
    "            gamma=0.99,\n",
    "            train_freq=1,\n",
    "            gradient_steps=1,\n",
    "            target_update_interval=500,\n",
    "            exploration_fraction=0.1,\n",
    "            exploration_initial_eps=1.0,\n",
    "            exploration_final_eps=0.05,\n",
    "            max_grad_norm=10,\n",
    "            verbose=0,\n",
    "            device=\"auto\"\n",
    "        )\n",
    "    \n",
    "    if model:\n",
    "        model.learn(total_timesteps=total_timesteps)\n",
    "        \n",
    "        # Evaluate\n",
    "        print(f\"Evaluating {agent_name}...\")\n",
    "        test_env = FraudDetectionEnv(X_test, y_test)\n",
    "        obs, _ = test_env.reset()\n",
    "        \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, truncated, info = test_env.step(action)\n",
    "            \n",
    "            if 'true_label' in info:\n",
    "                y_true.append(info['true_label'])\n",
    "                y_pred.append(action)\n",
    "                \n",
    "        # Metrics\n",
    "        print(f\"--- {agent_name} Results ---\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "        print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n",
    "        print(f\"Recall: {recall_score(y_true, y_pred):.4f}\")\n",
    "        print(f\"F1 Score: {f1_score(y_true, y_pred):.4f}\")\n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'{agent_name} Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "\n",
    "# Train Agents\n",
    "# Note: PPO parameters are default as it was not present in the reference notebook.\n",
    "train_evaluate_agent('PPO', X_train_aug, y_train_aug, X_test_pca, y_test)\n",
    "train_evaluate_agent('A2C', X_train_aug, y_train_aug, X_test_pca, y_test)\n",
    "train_evaluate_agent('DQN', X_train_aug, y_train_aug, X_test_pca, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
