{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28ntE059DoCg"
   },
   "source": [
    "## Tools and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qjW3KnJKwZIx"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5gzPBlNDWMw"
   },
   "source": [
    "## The Custom Environment\n",
    "\n",
    "This class acts as the interface between the dataset and the SB3 agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4q3XwHSUvb8a"
   },
   "outputs": [],
   "source": [
    "class FraudDetectionEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A custom Gym environment for Fraud Detection using embeddings.\n",
    "\n",
    "    State: Embedding of a transaction.\n",
    "    Action: 0 (Declare Not Fraud), 1 (Declare Fraud).\n",
    "    Reward: Based on correctly/incorrectly classifying fraud vs non-fraud.\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings: np.ndarray, labels: np.ndarray, reward_config: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure data consistency\n",
    "        assert embeddings.shape[0] == labels.shape[0], \"Embeddings and labels must have the same number of instances.\"\n",
    "        assert embeddings.shape[1] == 768, f\"Embeddings must be 768-dimensional, but got {embeddings.shape[1]}\"\n",
    "\n",
    "        self.embeddings = embeddings.astype(np.float32)\n",
    "        self.labels = labels.astype(np.int64)\n",
    "\n",
    "        self.num_instances = self.embeddings.shape[0]\n",
    "        self.reward_config = reward_config\n",
    "\n",
    "        # Define action and observation space\n",
    "        # Action Space: Discrete(2) -> 0 for Not Fraud, 1 for Fraud\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # Observation Space: Box(low, high, shape, dtype) -> 768-dim vector\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(768,), dtype=np.float32)\n",
    "\n",
    "        # Internal state\n",
    "        self._current_index = 0\n",
    "        self._order = np.arange(self.num_instances)\n",
    "        np.random.shuffle(self._order) # Shuffle the order of instances initially\n",
    "\n",
    "\n",
    "    def step(self, action: int):\n",
    "        # Check if episode is done\n",
    "        if self._current_index >= self.num_instances:\n",
    "            print(\"Warning: step() called when episode is already done.\")\n",
    "            return self.observation_space.sample() * 0, 0, True, False, {} # Return dummy values\n",
    "\n",
    "        # Get current instance data based on shuffled order\n",
    "        actual_index = self._order[self._current_index]\n",
    "        current_embedding = self.embeddings[actual_index]\n",
    "        true_label = self.labels[actual_index]\n",
    "\n",
    "        # Determine reward\n",
    "        reward = 0\n",
    "        if action == 1 and true_label == 1:\n",
    "            reward = self.reward_config.get('TP', 0)\n",
    "        elif action == 1 and true_label == 0:\n",
    "            reward = self.reward_config.get('FP', 0)\n",
    "        elif action == 0 and true_label == 1:\n",
    "            reward = self.reward_config.get('FN', 0)\n",
    "        elif action == 0 and true_label == 0:\n",
    "            reward = self.reward_config.get('TN', 0)\n",
    "\n",
    "        # Move to the next instance\n",
    "        self._current_index += 1\n",
    "\n",
    "        # Check if the episode is finished\n",
    "        done = self._current_index >= self.num_instances\n",
    "        truncated = False\n",
    "\n",
    "        # Get the next observation\n",
    "        next_observation = np.zeros_like(current_embedding, dtype=np.float32) # Default for done state\n",
    "        if not done:\n",
    "             next_observation = self.embeddings[self._order[self._current_index]]\n",
    "\n",
    "        info = {\n",
    "            'true_label': true_label,\n",
    "            'predicted_action': action,\n",
    "            'instance_uid': actual_index,\n",
    "            'is_done': done\n",
    "        }\n",
    "\n",
    "        return next_observation, reward, done, truncated, info\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed) # Handles seeding\n",
    "\n",
    "        # Reset index and shuffle order for a new episode\n",
    "        self._current_index = 0\n",
    "        self._order = np.arange(self.num_instances)\n",
    "        self.np_random.shuffle(self._order) # Use the environment's random number generator\n",
    "\n",
    "        # Get the first observation of the new episode\n",
    "        initial_observation = self.embeddings[self._order[self._current_index]]\n",
    "\n",
    "        info = {'instance_uid': self._order[self._current_index]}\n",
    "\n",
    "        return initial_observation, info\n",
    "\n",
    "    def close(self):\n",
    "        # Optional: Implement cleanup\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRZ90uK1FHne"
   },
   "source": [
    "## Loading the Data (Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2MwrlhFIFNvd"
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y20Dp-ctFtes",
    "outputId": "60eadd79-1ba0-414c-87ef-c24624ac1b17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: (2952, 768)\n",
      "True labels: (2952,)\n"
     ]
    }
   ],
   "source": [
    "embeddings = data['embeddings']\n",
    "labels = np.array(data['labels'])\n",
    "\n",
    "print(f\"Embeddings: {embeddings.shape}\")\n",
    "print(f\"True labels: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0KWfiKTnU1JE",
    "outputId": "69abf339-3200-42f7-da05-5a9d620bafe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Embeddings shape: (2361, 768)\n",
      "Training Labels shape: (2361,)\n",
      "Testing Embeddings shape: (591, 768)\n",
      "Testing Labels shape: (591,)\n",
      "\n",
      "Class distribution in splits:\n",
      "Original: Fraud=0.1667\n",
      "Train:    Fraud=0.1669\n",
      "Test:     Fraud=0.1658\n"
     ]
    }
   ],
   "source": [
    "embeddings_train, embeddings_test, labels_train, labels_test = train_test_split(\n",
    "    embeddings, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Training Embeddings shape: {embeddings_train.shape}\")\n",
    "print(f\"Training Labels shape: {labels_train.shape}\")\n",
    "\n",
    "print(f\"Testing Embeddings shape: {embeddings_test.shape}\")\n",
    "print(f\"Testing Labels shape: {labels_test.shape}\")\n",
    "\n",
    "print(\"\\nClass distribution in splits:\")\n",
    "print(f\"Original: Fraud={np.mean(labels):.4f}\")\n",
    "print(f\"Train:    Fraud={np.mean(labels_train):.4f}\")\n",
    "print(f\"Test:     Fraud={np.mean(labels_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7e_G0tWGS49"
   },
   "source": [
    "## Define Reward Configuration and Instantiate Environment\n",
    "\n",
    "Now we configure the rewards for the agent and create the environment instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93RpPrdiGanw",
    "outputId": "1f075237-9422-4a64-8edb-f5ca8535435c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward configuration: {'TP': 10.0, 'FP': -5.0, 'FN': -20.0, 'TN': 1.0}\n",
      "Training Environment created.\n",
      "Observation space: Box(-inf, inf, (768,), float32)\n",
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "reward_config = {\n",
    "    'TP': 10.0,\n",
    "    'FP': -5.0,\n",
    "    'FN': -20.0,\n",
    "    'TN': 1.0\n",
    "}\n",
    "\n",
    "print(\"Reward configuration:\", reward_config)\n",
    "\n",
    "# Create vectorized environment instance\n",
    "n_envs = 8\n",
    "train_env = make_vec_env(FraudDetectionEnv, env_kwargs={'embeddings': embeddings_train,\n",
    "                                                        'labels': labels_train,\n",
    "                                                        'reward_config': reward_config}, n_envs=n_envs, seed=0, vec_env_cls=SubprocVecEnv)\n",
    "\n",
    "\n",
    "print(\"Training Environment created.\")\n",
    "print(\"Observation space:\", train_env.observation_space)\n",
    "print(\"Action space:\", train_env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqZ5nOo5HmdI"
   },
   "source": [
    "## Define and Train the DQN Agent\n",
    "\n",
    "We define the DQN model and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_log_dir = \"./dqn_fraud_tb/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a learning rate schedule\n",
    "def linear_schedule(initial_value):\n",
    "    def schedule(progress_remaining):\n",
    "        return progress_remaining * initial_value\n",
    "    return schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LNpjLtHIHtu0",
    "outputId": "88e9285a-b01a-471f-c67a-d8649bde1046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "DQN model created.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the DQN model with MLP and scheduled learning rate\n",
    "model = DQN(\"MlpPolicy\",\n",
    "            train_env,\n",
    "            learning_rate=linear_schedule(1e-4),  # Use the schedule here\n",
    "            buffer_size=100000,  # Size of the replay buffer\n",
    "            learning_starts=1000, # Number of steps before learning starts (buffer needs data)\n",
    "            batch_size=512,      # Minibatch size for gradient updates\n",
    "            gamma=0.99,         # Discount factor\n",
    "            train_freq=1,       # Train the model after each episode step\n",
    "            gradient_steps=1,   # Number of gradient steps per training iteration\n",
    "            target_update_interval=500, # Update the target network every N steps\n",
    "            exploration_fraction=0.1, # Fraction of total timesteps for exploration phase\n",
    "            exploration_initial_eps=1.0, # Initial epsilon value\n",
    "            exploration_final_eps=0.05,  # Final epsilon value\n",
    "            max_grad_norm=10,   # Clip gradients to avoid instability\n",
    "            verbose=1,          # Print training information\n",
    "            device=\"auto\",      # Use GPU if available, otherwise CPU\n",
    "            tensorboard_log=tensorboard_log_dir, # Log to TensorBoard\n",
    "           )\n",
    "\n",
    "print(\"DQN model created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXYr5zORJxR8",
    "outputId": "68574c39-cfc4-444d-d4a3-8555519c03da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total episodes: 100\n",
      "Total timesteps: 236100\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 100 # Train for number passes through the data\n",
    "total_timesteps = total_episodes * embeddings_train.shape[0]\n",
    "\n",
    "print(f\"Total episodes: {total_episodes}\")\n",
    "print(f\"Total timesteps: {total_timesteps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_log_dir = \"./dqn_fraud_checkpoints/\" \n",
    "os.makedirs(checkpoint_log_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=checkpoint_log_dir, name_prefix='dqn_fraud_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "OjQwhu3ZKQ8_",
    "outputId": "be2b55b8-2aae-471e-dd64-cca017fc04e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./dqn_fraud_tb/DQN_7\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 2.36e+03  |\n",
      "|    ep_rew_mean      | -2.22e+03 |\n",
      "|    exploration_rate | 0.24      |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 2118      |\n",
      "|    time_elapsed     | 8         |\n",
      "|    total_timesteps  | 18888     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 9.2e-05   |\n",
      "|    loss             | 2.35      |\n",
      "|    n_updates        | 2235      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 2.36e+03  |\n",
      "|    ep_rew_mean     | -2.22e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 2117      |\n",
      "|    time_elapsed    | 8         |\n",
      "|    total_timesteps | 18888     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.36e+03 |\n",
      "|    ep_rew_mean      | 456      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 2131     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 37776    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 8.4e-05  |\n",
      "|    loss             | 1.89     |\n",
      "|    n_updates        | 4596     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.36e+03 |\n",
      "|    ep_rew_mean     | 456      |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 2131     |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 37776    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.36e+03 |\n",
      "|    ep_rew_mean      | 1.51e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 2124     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 56664    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.6e-05  |\n",
      "|    loss             | 2.11     |\n",
      "|    n_updates        | 6957     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.36e+03 |\n",
      "|    ep_rew_mean     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 2124     |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 56664    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.36e+03 |\n",
      "|    ep_rew_mean      | 2.05e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 2110     |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 75552    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6.8e-05  |\n",
      "|    loss             | 1.62     |\n",
      "|    n_updates        | 9318     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.36e+03 |\n",
      "|    ep_rew_mean     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 2110     |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 75552    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.36e+03 |\n",
      "|    ep_rew_mean      | 2.39e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 2097     |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 94440    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 6e-05    |\n",
      "|    loss             | 1.55     |\n",
      "|    n_updates        | 11679    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.36e+03 |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 2097     |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 94440    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.36e+03 |\n",
      "|    ep_rew_mean      | 2.61e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 2103     |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 113328   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 5.2e-05  |\n",
      "|    loss             | 1.59     |\n",
      "|    n_updates        | 14040    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.36e+03 |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 2103     |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total_timesteps | 113328   |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.36e+03 |\n",
      "|    ep_rew_mean      | 2.79e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 2111     |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 132216   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.4e-05  |\n",
      "|    loss             | 1.87     |\n",
      "|    n_updates        | 16401    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.36e+03 |\n",
      "|    ep_rew_mean     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 2111     |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 132216   |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.36e+03 |\n",
      "|    ep_rew_mean      | 2.92e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 2115     |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 151104   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 3.6e-05  |\n",
      "|    loss             | 1.76     |\n",
      "|    n_updates        | 18762    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.36e+03 |\n",
      "|    ep_rew_mean     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 2115     |\n",
      "|    time_elapsed    | 71       |\n",
      "|    total_timesteps | 151104   |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.36e+03 |\n",
      "|    ep_rew_mean      | 3.04e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 2119     |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 169992   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.8e-05  |\n",
      "|    loss             | 1.84     |\n",
      "|    n_updates        | 21123    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.36e+03 |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 2119     |\n",
      "|    time_elapsed    | 80       |\n",
      "|    total_timesteps | 169992   |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.36e+03 |\n",
      "|    ep_rew_mean      | 3.13e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 2119     |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 188880   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2e-05    |\n",
      "|    loss             | 1.67     |\n",
      "|    n_updates        | 23484    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.36e+03 |\n",
      "|    ep_rew_mean     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 2119     |\n",
      "|    time_elapsed    | 89       |\n",
      "|    total_timesteps | 188880   |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.36e+03 |\n",
      "|    ep_rew_mean      | 3.2e+03  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 2118     |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 207768   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 1.2e-05  |\n",
      "|    loss             | 1.69     |\n",
      "|    n_updates        | 25845    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.36e+03 |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 2118     |\n",
      "|    time_elapsed    | 98       |\n",
      "|    total_timesteps | 207768   |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.36e+03 |\n",
      "|    ep_rew_mean      | 3.26e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 2117     |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 226656   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4e-06    |\n",
      "|    loss             | 1.79     |\n",
      "|    n_updates        | 28206    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.36e+03 |\n",
      "|    ep_rew_mean     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 2117     |\n",
      "|    time_elapsed    | 107      |\n",
      "|    total_timesteps | 226656   |\n",
      "---------------------------------\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.learn(total_timesteps=total_timesteps, callback=checkpoint_callback)\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aykv3BchQrdQ"
   },
   "source": [
    "## Evaluate the Trained DQN Agent\n",
    "\n",
    "After training, we want to see how well the agent performs. We'll run it without exploration\n",
    "\n",
    "\n",
    "\n",
    "**Note: We create a separate environment instance using the test data and run the evaluation loop on it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBnvzVpTW0F7",
    "outputId": "e1515dd2-132e-44df-bd50-43e6c08d79f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Environment created\n"
     ]
    }
   ],
   "source": [
    "eval_env = make_vec_env(FraudDetectionEnv,\n",
    "                        env_kwargs={'embeddings': embeddings_test,\n",
    "                                    'labels': labels_test,\n",
    "                                    'reward_config': reward_config},\n",
    "                        n_envs=1,\n",
    "                        seed=43)\n",
    "print(\"Evaluation Environment created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logs will be saved to: ./dqn_fraud_tb/evaluation/eval_20251201-112610\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "eval_log_dir = os.path.join(tensorboard_log_dir, \"evaluation\", f\"eval_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "writer = SummaryWriter(eval_log_dir)\n",
    "print(f\"TensorBoard logs will be saved to: {eval_log_dir}\")\n",
    "\n",
    "rewards_over_time = []\n",
    "\n",
    "# Reset the environment for evaluation\n",
    "obs = eval_env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "instance_uids = []\n",
    "episode_steps = 0\n",
    "all_q_values = []\n",
    "rewards_over_time = []  # Track rewards for plotting\n",
    "\n",
    "# Use deterministic=True to turn off exploration (epsilon = 0)\n",
    "while not done:\n",
    "    # For Stable Baselines3 DQN, we can access q_values directly\n",
    "    with torch.no_grad():\n",
    "        # Convert observation to tensor\n",
    "        obs_tensor = torch.FloatTensor(obs).to(model.device)\n",
    "        # Get q_values using SB3's policy (this works for DQN)\n",
    "        q_values = model.q_net(obs_tensor).detach().cpu().numpy()\n",
    "    \n",
    "    all_q_values.append(q_values)\n",
    "    \n",
    "    # Predict action\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    action = action[0]\n",
    "    \n",
    "    # Step the environment\n",
    "    obs, reward, done_flags, infos = eval_env.step([action])\n",
    "    \n",
    "    # Extract info for the single environment\n",
    "    info = infos[0]\n",
    "    done = done_flags[0]\n",
    "    reward_value = reward[0]  # Reward is also a batch for VecEnv\n",
    "    total_reward += reward_value\n",
    "    rewards_over_time.append(reward_value)  # Store reward for plotting\n",
    "    \n",
    "    # Store results\n",
    "    predicted_labels.append(action)\n",
    "    true_labels.append(info['true_label'])\n",
    "    instance_uids.append(info['instance_uid'])\n",
    "    \n",
    "    # Log immediate reward for this step\n",
    "    writer.add_scalar('Evaluation/Step_Reward', reward_value, episode_steps)\n",
    "    \n",
    "    # Log Q-values for this step\n",
    "    for i, q_val in enumerate(q_values[0]):\n",
    "        writer.add_scalar(f'Evaluation/Q_Value_Action_{i}', q_val, episode_steps)\n",
    "    \n",
    "    # Log max Q-value\n",
    "    writer.add_scalar('Evaluation/Max_Q_Value', np.max(q_values), episode_steps)\n",
    "    \n",
    "    # Log the chosen action\n",
    "    writer.add_scalar('Evaluation/Action_Taken', action, episode_steps)\n",
    "    \n",
    "    episode_steps += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation finished. Total reward: 1017.0\n",
      "Accuracy: 0.9459\n",
      "TensorBoard logs saved to ./dqn_fraud_tb/evaluation/eval_20251201-112610\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute accuracy\n",
    "accuracy = np.mean(np.array(predicted_labels) == np.array(true_labels))\n",
    "writer.add_scalar('Evaluation/Accuracy', accuracy, 0)\n",
    "writer.add_scalar('Evaluation/Total_Reward', total_reward, 0)\n",
    "writer.add_scalar('Evaluation/Steps', episode_steps, 0)\n",
    "\n",
    "# Create and log confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Convert plot to image and log to TensorBoard\n",
    "buf = io.BytesIO()\n",
    "plt.savefig(buf, format='png')\n",
    "buf.seek(0)\n",
    "img = Image.open(buf)\n",
    "img_tensor = np.array(img).transpose((2, 0, 1))  # Convert to CHW format\n",
    "writer.add_image('Evaluation/Confusion_Matrix', img_tensor, 0)\n",
    "plt.close()\n",
    "\n",
    "# Log detailed classification metrics\n",
    "class_names = [f\"Class_{i}\" for i in range(max(max(true_labels), max(predicted_labels)) + 1)]\n",
    "report = classification_report(true_labels, predicted_labels, target_names=class_names)\n",
    "writer.add_text('Evaluation/Classification_Report', '```\\n' + report + '\\n```', 0)\n",
    "\n",
    "# Add class-wise metrics\n",
    "for cls in range(len(class_names)):\n",
    "    # Calculate class precision and recall\n",
    "    true_positives = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == cls and pred == cls)\n",
    "    predicted_as_cls = sum(1 for pred in predicted_labels if pred == cls)\n",
    "    actual_cls = sum(1 for true in true_labels if true == cls)\n",
    "    \n",
    "    precision = true_positives / predicted_as_cls if predicted_as_cls > 0 else 0\n",
    "    recall = true_positives / actual_cls if actual_cls > 0 else 0\n",
    "    \n",
    "    writer.add_scalar(f'Evaluation/Class_{cls}_Precision', precision, 0)\n",
    "    writer.add_scalar(f'Evaluation/Class_{cls}_Recall', recall, 0)\n",
    "\n",
    "# Visualize average Q-values across the episode\n",
    "all_q_values = np.array(all_q_values)\n",
    "avg_q_values = np.mean(all_q_values, axis=0)[0]  # Average across steps\n",
    "writer.add_scalar('Evaluation/Average_Max_Q_Value', np.max(avg_q_values), 0)\n",
    "\n",
    "# Create bar plot of average Q-values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(avg_q_values)), avg_q_values)\n",
    "plt.xlabel('Actions')\n",
    "plt.ylabel('Average Q-Value')\n",
    "plt.title('Average Q-Values Per Action')\n",
    "\n",
    "# Convert plot to image and log to TensorBoard\n",
    "buf = io.BytesIO()\n",
    "plt.savefig(buf, format='png')\n",
    "buf.seek(0)\n",
    "img = Image.open(buf)\n",
    "img_tensor = np.array(img).transpose((2, 0, 1))\n",
    "writer.add_image('Evaluation/Average_Q_Values', img_tensor, 0)\n",
    "plt.close()\n",
    "\n",
    "# Create a plot of rewards over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(len(rewards_over_time)), rewards_over_time)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Step Reward')\n",
    "plt.title('Rewards Per Step During Evaluation')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "cumulative_rewards = np.cumsum(rewards_over_time)\n",
    "plt.plot(range(len(cumulative_rewards)), cumulative_rewards)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Cumulative Reward During Evaluation')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Convert plot to image and log to TensorBoard\n",
    "buf = io.BytesIO()\n",
    "plt.savefig(buf, format='png')\n",
    "buf.seek(0)\n",
    "img = Image.open(buf)\n",
    "img_tensor = np.array(img).transpose((2, 0, 1))\n",
    "writer.add_image('Evaluation/Reward_Analysis', img_tensor, 0)\n",
    "plt.close()\n",
    "\n",
    "# Close the writer\n",
    "writer.close()\n",
    "\n",
    "print(f\"Evaluation finished. Total reward: {total_reward}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"TensorBoard logs saved to {eval_log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pT8sZOCRQvzk",
    "outputId": "7709482d-132d-43d2-fcbd-fe7ecf2b9193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation finished. Total reward: 1017.0\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment for evaluation\n",
    "# The environment will shuffle the data again for the evaluation run\n",
    "obs = eval_env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "instance_uids = []\n",
    "\n",
    "\n",
    "# Use deterministic=True to turn off exploration (epsilon = 0)\n",
    "while not done:\n",
    "\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "\n",
    "    action = action[0]\n",
    "\n",
    "    # Step the environment\n",
    "    obs, reward, done_flags, infos = eval_env.step([action])\n",
    "\n",
    "    # Extract info for the single environment\n",
    "    info = infos[0]\n",
    "    done = done_flags[0]\n",
    "\n",
    "    total_reward += reward[0] # Reward is also a batch for VecEnv\n",
    "\n",
    "    # Store results\n",
    "    predicted_labels.append(action)\n",
    "    true_labels.append(info['true_label'])\n",
    "    instance_uids.append(info['instance_uid'])\n",
    "\n",
    "print(f\"Evaluation finished. Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "id": "TADHJk5CSb_q",
    "outputId": "3b1ed23a-8b5c-493c-9de3-cae0161355a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Classification Metrics ---\n",
      "Confusion Matrix:\n",
      "TP: 87, FP: 21, FN: 11, TN: 472\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_labels = np.array(predicted_labels)\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "TN, FP, FN, TP = cm.ravel() if cm.size == 4 else (0,0,0,0)\n",
    "\n",
    "print(\"\\n--- Classification Metrics ---\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"TP: {TP}, FP: {FP}, FN: {FN}, TN: {TN}\")\n",
    "\n",
    "\n",
    "#plot the confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JeDfCcL4SwWk",
    "outputId": "2c81e370-c2dc-4147-e7db-a72704580ed4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9459\n",
      "Precision (Class 1 - Fraud): 0.8056\n",
      "Recall (Class 1 - Fraud): 0.8878\n",
      "F1 Score (Class 1 - Fraud): 0.8447\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, zero_division=0)\n",
    "recall = recall_score(true_labels, predicted_labels, zero_division=0)\n",
    "f1 = f1_score(true_labels, predicted_labels, zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision (Class 1 - Fraud): {precision:.4f}\")\n",
    "print(f\"Recall (Class 1 - Fraud): {recall:.4f}\")\n",
    "print(f\"F1 Score (Class 1 - Fraud): {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nR-wIwwda6ny"
   },
   "source": [
    "Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLLONefia_Kj",
    "outputId": "4e060e47-ff20-4ed8-cdb7-0d3db73aaced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved to .models/dqn_fraud_model\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"models/dqn_fraud_model\"\n",
    "model.save(model_save_path)\n",
    "print(f\"\\nModel saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BR06GGNOwMUc"
   },
   "source": [
    "## THE ACTOR CRITIC MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "_tt23-NNv-Tf"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Environment reset.\n"
     ]
    }
   ],
   "source": [
    "tensorboard_log_dir2 = \"./a2c_fraud_tb/\"\n",
    "train_env.reset()\n",
    "print(\"Training Environment reset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_log_dir2 = \"./a2c_fraud_checkpoints/\" \n",
    "os.makedirs(checkpoint_log_dir2, exist_ok=True)\n",
    "\n",
    "checkpoint_callback2 = CheckpointCallback(save_freq=10000, save_path=checkpoint_log_dir2, name_prefix='A2C_fraud_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qstVKRacwd7b",
    "outputId": "dad65f42-9d95-4081-fe9a-bb3e9f964ee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "A2C model created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djalal/.pyvenv/lib/python3.13/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the A2C model with MLP policy\n",
    "ac_model = A2C(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    learning_rate=1e-4,\n",
    "    gamma=0.99,          # discount factor\n",
    "    n_steps=5,           # how many steps to run for each update\n",
    "    ent_coef=0.01,       # entropy bonus\n",
    "    vf_coef=0.5,         # value function loss coefficient\n",
    "    max_grad_norm=0.5,   # gradient clipping\n",
    "    verbose=1,           # print training info\n",
    "    device=\"auto\",        # GPU if available\n",
    "    tensorboard_log=tensorboard_log_dir2, # log to TensorBoard\n",
    "    \n",
    ")\n",
    "print(\"A2C model created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CyQ7abFxwgUF",
    "outputId": "6540f99c-9e41-4eea-d5b6-6e3a1718d8cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./a2c_fraud_tb/A2C_4\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 2781     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.337   |\n",
      "|    explained_variance | 0.0299   |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -2.08    |\n",
      "|    value_loss         | 140      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 3380     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.22    |\n",
      "|    explained_variance | 0.0372   |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -1.19    |\n",
      "|    value_loss         | 25.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 3637     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.146   |\n",
      "|    explained_variance | 0.000579 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.113   |\n",
      "|    value_loss         | 59.5     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 3773     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.101   |\n",
      "|    explained_variance | 0.000506 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.386    |\n",
      "|    value_loss         | 180      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 1.99e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 3871     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.113   |\n",
      "|    explained_variance | 0.000544 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.748   |\n",
      "|    value_loss         | 91.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 1.99e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 3929     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0963  |\n",
      "|    explained_variance | 0.000254 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.244    |\n",
      "|    value_loss         | 73.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 1.99e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 3981     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 28000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0656  |\n",
      "|    explained_variance | 0.000515 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.0811   |\n",
      "|    value_loss         | 55.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 1.99e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4014     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.059   |\n",
      "|    explained_variance | 0.000181 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.0708   |\n",
      "|    value_loss         | 109      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 1.99e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4040     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0484  |\n",
      "|    explained_variance | 5.61e-05 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.14     |\n",
      "|    value_loss         | 57.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 2.74e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4065     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0286  |\n",
      "|    explained_variance | 3.24e-05 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.024    |\n",
      "|    value_loss         | 76.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 2.74e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4090     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0411  |\n",
      "|    explained_variance | 4.35e-05 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.00504 |\n",
      "|    value_loss         | 36.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 2.74e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4108     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0335  |\n",
      "|    explained_variance | 1.23e-05 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.0653   |\n",
      "|    value_loss         | 112      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 2.74e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4117     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 52000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0262  |\n",
      "|    explained_variance | 5.07e-06 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.116    |\n",
      "|    value_loss         | 56.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 2.74e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4130     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 56000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0213  |\n",
      "|    explained_variance | 1.43e-06 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 1.9      |\n",
      "|    value_loss         | 109      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.02e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4134     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 60000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0459  |\n",
      "|    explained_variance | 4.05e-06 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.0773  |\n",
      "|    value_loss         | 66.7     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 2.36e+03  |\n",
      "|    ep_rew_mean        | 3.02e+03  |\n",
      "| time/                 |           |\n",
      "|    fps                | 4142      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 64000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0306   |\n",
      "|    explained_variance | -8.34e-07 |\n",
      "|    learning_rate      | 0.0001    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 0.965     |\n",
      "|    value_loss         | 70.4      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.02e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4146     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 68000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0224  |\n",
      "|    explained_variance | 7.75e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.686    |\n",
      "|    value_loss         | 100      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.02e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4153     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 72000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0455  |\n",
      "|    explained_variance | 1.13e-06 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.0387  |\n",
      "|    value_loss         | 96.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.19e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4160     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 76000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0103  |\n",
      "|    explained_variance | 4.77e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.00211  |\n",
      "|    value_loss         | 113      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.19e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4163     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0531  |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.571    |\n",
      "|    value_loss         | 197      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.19e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4167     |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 84000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0275  |\n",
      "|    explained_variance | 9.54e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | -0.121   |\n",
      "|    value_loss         | 107      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.19e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4171     |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 88000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0444  |\n",
      "|    explained_variance | 7.75e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | -0.125   |\n",
      "|    value_loss         | 56.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.19e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4176     |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 92000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0822  |\n",
      "|    explained_variance | 6.56e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | -0.754   |\n",
      "|    value_loss         | 127      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.3e+03  |\n",
      "| time/                 |          |\n",
      "|    fps                | 4179     |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0299  |\n",
      "|    explained_variance | 4.17e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 0.139    |\n",
      "|    value_loss         | 101      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.3e+03  |\n",
      "| time/                 |          |\n",
      "|    fps                | 4182     |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 100000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0562  |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | -0.423   |\n",
      "|    value_loss         | 74.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.3e+03  |\n",
      "| time/                 |          |\n",
      "|    fps                | 4184     |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 104000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0108  |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | 0.0042   |\n",
      "|    value_loss         | 26.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.3e+03  |\n",
      "| time/                 |          |\n",
      "|    fps                | 4188     |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 108000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0916  |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | -0.484   |\n",
      "|    value_loss         | 88       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.3e+03  |\n",
      "| time/                 |          |\n",
      "|    fps                | 4194     |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 112000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0247  |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | -0.0237  |\n",
      "|    value_loss         | 51.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.4e+03  |\n",
      "| time/                 |          |\n",
      "|    fps                | 4197     |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 116000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0129  |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 0.00277  |\n",
      "|    value_loss         | 49.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.4e+03  |\n",
      "| time/                 |          |\n",
      "|    fps                | 4199     |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 120000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.025   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | -0.0665  |\n",
      "|    value_loss         | 92.2     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 2.36e+03  |\n",
      "|    ep_rew_mean        | 3.4e+03   |\n",
      "| time/                 |           |\n",
      "|    fps                | 4199      |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 124000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0581   |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0001    |\n",
      "|    n_updates          | 3099      |\n",
      "|    policy_loss        | -0.233    |\n",
      "|    value_loss         | 44.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 2.36e+03  |\n",
      "|    ep_rew_mean        | 3.4e+03   |\n",
      "| time/                 |           |\n",
      "|    fps                | 4201      |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 128000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0288   |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0001    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | 0.109     |\n",
      "|    value_loss         | 53.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.4e+03  |\n",
      "| time/                 |          |\n",
      "|    fps                | 4205     |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 132000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0424  |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | -0.259   |\n",
      "|    value_loss         | 83.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.49e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4207     |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 136000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0409  |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | -0.38    |\n",
      "|    value_loss         | 99.3     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 2.36e+03  |\n",
      "|    ep_rew_mean        | 3.49e+03  |\n",
      "| time/                 |           |\n",
      "|    fps                | 4209      |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 140000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0569   |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0001    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | -0.77     |\n",
      "|    value_loss         | 42.2      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.49e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4211     |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 34       |\n",
      "|    total_timesteps    | 144000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0195  |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | 0.0311   |\n",
      "|    value_loss         | 149      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.49e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4213     |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 148000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0362  |\n",
      "|    explained_variance | 5.96e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | -0.26    |\n",
      "|    value_loss         | 49.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.56e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4217     |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 152000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0411  |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | 0.322    |\n",
      "|    value_loss         | 84.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.56e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4220     |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 156000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0581  |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 0.249    |\n",
      "|    value_loss         | 91.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.56e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4223     |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 37       |\n",
      "|    total_timesteps    | 160000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0426  |\n",
      "|    explained_variance | 3.58e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | -0.211   |\n",
      "|    value_loss         | 74.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.56e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4226     |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 38       |\n",
      "|    total_timesteps    | 164000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0729  |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | -0.00721 |\n",
      "|    value_loss         | 148      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.56e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4229     |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 39       |\n",
      "|    total_timesteps    | 168000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00908 |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | 0.0111   |\n",
      "|    value_loss         | 91.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.62e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4232     |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 40       |\n",
      "|    total_timesteps    | 172000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0366  |\n",
      "|    explained_variance | 2.98e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | -0.326   |\n",
      "|    value_loss         | 74.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.62e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4236     |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 41       |\n",
      "|    total_timesteps    | 176000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0149  |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | 0.0106   |\n",
      "|    value_loss         | 54.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.62e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4238     |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 42       |\n",
      "|    total_timesteps    | 180000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00622 |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | 0.00131  |\n",
      "|    value_loss         | 38.5     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.62e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4240     |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 43       |\n",
      "|    total_timesteps    | 184000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0242  |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | -0.367   |\n",
      "|    value_loss         | 171      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.62e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4241     |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 44       |\n",
      "|    total_timesteps    | 188000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00853 |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | 0.0162   |\n",
      "|    value_loss         | 54.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.68e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4242     |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 45       |\n",
      "|    total_timesteps    | 192000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0476  |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | 0.353    |\n",
      "|    value_loss         | 43.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.68e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4243     |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 46       |\n",
      "|    total_timesteps    | 196000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00256 |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | 0.00263  |\n",
      "|    value_loss         | 159      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.68e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4243     |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 47       |\n",
      "|    total_timesteps    | 200000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0257  |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | -0.0432  |\n",
      "|    value_loss         | 102      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.68e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4243     |\n",
      "|    iterations         | 5100     |\n",
      "|    time_elapsed       | 48       |\n",
      "|    total_timesteps    | 204000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0101  |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | 0.0301   |\n",
      "|    value_loss         | 147      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.72e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4242     |\n",
      "|    iterations         | 5200     |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 208000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00355 |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | 0.00509  |\n",
      "|    value_loss         | 58.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.72e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4240     |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 212000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0148  |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | 0.00866  |\n",
      "|    value_loss         | 30.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.72e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4241     |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 50       |\n",
      "|    total_timesteps    | 216000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00319 |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | 0.00151  |\n",
      "|    value_loss         | 121      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.72e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4243     |\n",
      "|    iterations         | 5500     |\n",
      "|    time_elapsed       | 51       |\n",
      "|    total_timesteps    | 220000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0208  |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | 0.042    |\n",
      "|    value_loss         | 69.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.72e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4245     |\n",
      "|    iterations         | 5600     |\n",
      "|    time_elapsed       | 52       |\n",
      "|    total_timesteps    | 224000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0322  |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | 0.519    |\n",
      "|    value_loss         | 96.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.76e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4247     |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 53       |\n",
      "|    total_timesteps    | 228000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.021   |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | 0.019    |\n",
      "|    value_loss         | 106      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 2.36e+03  |\n",
      "|    ep_rew_mean        | 3.76e+03  |\n",
      "| time/                 |           |\n",
      "|    fps                | 4248      |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 54        |\n",
      "|    total_timesteps    | 232000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0778   |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0001    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | -0.196    |\n",
      "|    value_loss         | 36.4      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 2.36e+03 |\n",
      "|    ep_rew_mean        | 3.76e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 4248     |\n",
      "|    iterations         | 5900     |\n",
      "|    time_elapsed       | 55       |\n",
      "|    total_timesteps    | 236000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0357  |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | -0.0438  |\n",
      "|    value_loss         | 22.5     |\n",
      "------------------------------------\n",
      "A2C training finished.\n"
     ]
    }
   ],
   "source": [
    "# Reuse the same total_timesteps you set for DQN\n",
    "ac_model.learn(total_timesteps=total_timesteps, callback=checkpoint_callback2)\n",
    "print(\"A2C training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sebQ3MCnwl3Q",
    "outputId": "144040db-b48b-47da-861c-c2d584929ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2C Evaluation finished. Total reward: 1029.0\n"
     ]
    }
   ],
   "source": [
    "# Reset the eval environment\n",
    "obs = eval_env.reset()\n",
    "done = False\n",
    "predicted_labels_ac = []\n",
    "true_labels_ac = []\n",
    "instance_uids_ac = []\n",
    "total_reward_ac = 0\n",
    "\n",
    "# Turn off exploration for evaluation\n",
    "while not done:\n",
    "    action, _ = ac_model.predict(obs, deterministic=True)\n",
    "    action = action[0]\n",
    "    obs, reward, done_flags, infos = eval_env.step([action])\n",
    "    info = infos[0]\n",
    "    done = done_flags[0]\n",
    "    total_reward_ac += reward[0]\n",
    "    predicted_labels_ac.append(action)\n",
    "    true_labels_ac.append(info['true_label'])\n",
    "    instance_uids_ac.append(info['instance_uid'])\n",
    "\n",
    "print(f\"A2C Evaluation finished. Total reward: {total_reward_ac}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "FCPukS5pwnLj",
    "outputId": "b6d8ccd4-994b-41d6-ad52-b14e73a05231"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- A2C Classification Metrics ---\n",
      "TP: 89, FP: 29, FN: 9, TN: 464\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare arrays\n",
    "predicted_labels_ac = np.array(predicted_labels_ac)\n",
    "true_labels_ac = np.array(true_labels_ac)\n",
    "\n",
    "# Compute matrix\n",
    "cm_ac = confusion_matrix(true_labels_ac, predicted_labels_ac)\n",
    "TN, FP, FN, TP = cm_ac.ravel() if cm_ac.size == 4 else (0,0,0,0)\n",
    "\n",
    "print(\"\\n--- A2C Classification Metrics ---\")\n",
    "print(f\"TP: {TP}, FP: {FP}, FN: {FN}, TN: {TN}\")\n",
    "\n",
    "# Plot\n",
    "sns.heatmap(cm_ac, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('A2C Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logs will be saved to: ./a2c_fraud_tb/evaluation/a2c_eval_20251201-112708\n",
      "Evaluation finished. Total reward: 1029.0\n",
      "Accuracy: 0.9357\n",
      "TensorBoard logs saved to ./a2c_fraud_tb/evaluation/a2c_eval_20251201-112708\n",
      "To view TensorBoard visualization, run:\n",
      "tensorboard --logdir=./dqn_fraud_tb/\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Create TensorBoard writer - using the same log dir as during training\n",
    "# but in a separate \"evaluation\" subfolder\n",
    "eval_log_dir = os.path.join(tensorboard_log_dir2, \"evaluation\", f\"a2c_eval_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "writer = SummaryWriter(eval_log_dir)\n",
    "print(f\"TensorBoard logs will be saved to: {eval_log_dir}\")\n",
    "\n",
    "# Lists to store data for visualization\n",
    "rewards_over_time = []\n",
    "values_over_time = []\n",
    "action_probs_over_time = []\n",
    "entropies = []\n",
    "\n",
    "# Reset the environment for evaluation\n",
    "obs = eval_env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "instance_uids = []\n",
    "episode_steps = 0\n",
    "\n",
    "# Use deterministic=True to turn off exploration\n",
    "while not done:\n",
    "    # For A2C, we can get action probabilities and state values\n",
    "    with torch.no_grad():\n",
    "        # Convert observation to tensor\n",
    "        obs_tensor = torch.FloatTensor(obs).to(ac_model.policy.device)\n",
    "        \n",
    "        # Access the policy network and value network directly\n",
    "        # This is the correct way to access the policy in SB3 A2C\n",
    "        features = ac_model.policy.extract_features(obs_tensor)\n",
    "        \n",
    "        # Get action distribution\n",
    "        latent_pi, latent_vf = ac_model.policy.mlp_extractor(features)\n",
    "        action_logits = ac_model.policy.action_net(latent_pi)\n",
    "        \n",
    "        # Calculate action probabilities (softmax of logits for discrete actions)\n",
    "        action_probs = torch.softmax(action_logits, dim=1)\n",
    "        \n",
    "        # Get value function prediction\n",
    "        values = ac_model.policy.value_net(latent_vf)\n",
    "        \n",
    "        # Calculate entropy (measure of exploration)\n",
    "        # For categorical/discrete actions, entropy is -sum(p*log(p))\n",
    "        log_probs = torch.log_softmax(action_logits, dim=1)\n",
    "        entropy = -torch.sum(action_probs * log_probs, dim=1).mean().item()\n",
    "    \n",
    "    # Store data for visualization\n",
    "    action_probs_over_time.append(action_probs.cpu().numpy()[0])\n",
    "    values_over_time.append(values.cpu().numpy()[0][0])\n",
    "    entropies.append(entropy)\n",
    "    \n",
    "    # Predict action (deterministic = use most probable action)\n",
    "    action, _ = ac_model.predict(obs, deterministic=True)\n",
    "    action = action[0]\n",
    "    \n",
    "    # Log action probabilities\n",
    "    for i, prob in enumerate(action_probs.cpu().numpy()[0]):\n",
    "        writer.add_scalar(f'Evaluation/Action_Prob_{i}', prob, episode_steps)\n",
    "    \n",
    "    # Log value function and entropy\n",
    "    writer.add_scalar('Evaluation/Value_Function', values.cpu().numpy()[0][0], episode_steps)\n",
    "    writer.add_scalar('Evaluation/Policy_Entropy', entropy, episode_steps)\n",
    "    \n",
    "    # Step the environment\n",
    "    obs, reward, done_flags, infos = eval_env.step([action])\n",
    "    \n",
    "    # Extract info for the single environment\n",
    "    info = infos[0]\n",
    "    done = done_flags[0]\n",
    "    reward_value = reward[0]  # Reward is also a batch for VecEnv\n",
    "    total_reward += reward_value\n",
    "    rewards_over_time.append(reward_value)\n",
    "    \n",
    "    # Store results\n",
    "    predicted_labels.append(action)\n",
    "    true_labels.append(info['true_label'])\n",
    "    instance_uids.append(info['instance_uid'])\n",
    "    \n",
    "    # Log immediate reward for this step\n",
    "    writer.add_scalar('Evaluation/Step_Reward', reward_value, episode_steps)\n",
    "    \n",
    "    # Log the chosen action\n",
    "    writer.add_scalar('Evaluation/Action_Taken', action, episode_steps)\n",
    "    \n",
    "    episode_steps += 1\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = np.mean(np.array(predicted_labels) == np.array(true_labels))\n",
    "writer.add_scalar('Evaluation/Accuracy', accuracy, 0)\n",
    "writer.add_scalar('Evaluation/Total_Reward', total_reward, 0)\n",
    "writer.add_scalar('Evaluation/Steps', episode_steps, 0)\n",
    "\n",
    "# Create and log confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Convert plot to image and log to TensorBoard\n",
    "buf = io.BytesIO()\n",
    "plt.savefig(buf, format='png')\n",
    "buf.seek(0)\n",
    "img = Image.open(buf)\n",
    "img_tensor = np.array(img).transpose((2, 0, 1))  # Convert to CHW format\n",
    "writer.add_image('Evaluation/Confusion_Matrix', img_tensor, 0)\n",
    "plt.close()\n",
    "\n",
    "# Log detailed classification metrics\n",
    "class_names = [f\"Class_{i}\" for i in range(max(max(true_labels), max(predicted_labels)) + 1)]\n",
    "report = classification_report(true_labels, predicted_labels, target_names=class_names)\n",
    "writer.add_text('Evaluation/Classification_Report', '```\\n' + report + '\\n```', 0)\n",
    "\n",
    "# Add class-wise metrics\n",
    "for cls in range(len(class_names)):\n",
    "    # Calculate class precision and recall\n",
    "    true_positives = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == cls and pred == cls)\n",
    "    predicted_as_cls = sum(1 for pred in predicted_labels if pred == cls)\n",
    "    actual_cls = sum(1 for true in true_labels if true == cls)\n",
    "    \n",
    "    precision = true_positives / predicted_as_cls if predicted_as_cls > 0 else 0\n",
    "    recall = true_positives / actual_cls if actual_cls > 0 else 0\n",
    "    \n",
    "    writer.add_scalar(f'Evaluation/Class_{cls}_Precision', precision, 0)\n",
    "    writer.add_scalar(f'Evaluation/Class_{cls}_Recall', recall, 0)\n",
    "\n",
    "# Create a plot of rewards and value function over time\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot rewards\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(range(len(rewards_over_time)), rewards_over_time)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Step Reward')\n",
    "plt.title('Rewards Per Step')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot cumulative rewards\n",
    "plt.subplot(2, 2, 2)\n",
    "cumulative_rewards = np.cumsum(rewards_over_time)\n",
    "plt.plot(range(len(cumulative_rewards)), cumulative_rewards)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Cumulative Reward')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot value function estimates\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(range(len(values_over_time)), values_over_time)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Value Estimate')\n",
    "plt.title('Value Function Estimates')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot advantage (reward - value) to see if value function is accurate\n",
    "plt.subplot(2, 2, 4)\n",
    "advantages = [r - v for r, v in zip(rewards_over_time, values_over_time)]\n",
    "plt.plot(range(len(advantages)), advantages)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Advantage (Reward - Value)')\n",
    "plt.title('Advantage Estimates')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Convert plot to image and log to TensorBoard\n",
    "buf = io.BytesIO()\n",
    "plt.savefig(buf, format='png')\n",
    "buf.seek(0)\n",
    "img = Image.open(buf)\n",
    "img_tensor = np.array(img).transpose((2, 0, 1))\n",
    "writer.add_image('Evaluation/Reward_Value_Analysis', img_tensor, 0)\n",
    "plt.close()\n",
    "\n",
    "# Visualize action probabilities over time (policy evolution)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot action probabilities\n",
    "plt.subplot(1, 2, 1)\n",
    "action_probs_array = np.array(action_probs_over_time)\n",
    "for i in range(action_probs_array.shape[1]):\n",
    "    plt.plot(range(len(action_probs_over_time)), action_probs_array[:, i], label=f'Action {i}')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Action Probability')\n",
    "plt.title('Action Probabilities Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot entropy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(entropies)), entropies)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Entropy')\n",
    "plt.title('Policy Entropy Over Time')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Convert plot to image and log to TensorBoard\n",
    "buf = io.BytesIO()\n",
    "plt.savefig(buf, format='png')\n",
    "buf.seek(0)\n",
    "img = Image.open(buf)\n",
    "img_tensor = np.array(img).transpose((2, 0, 1))\n",
    "writer.add_image('Evaluation/Policy_Analysis', img_tensor, 0)\n",
    "plt.close()\n",
    "\n",
    "# Close the writer\n",
    "writer.close()\n",
    "\n",
    "print(f\"Evaluation finished. Total reward: {total_reward}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"TensorBoard logs saved to {eval_log_dir}\")\n",
    "print(\"To view TensorBoard visualization, run:\")\n",
    "print(f\"tensorboard --logdir={tensorboard_log_dir}\")  # Point to the parent log dir to see both training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Am0QewfkwqZS",
    "outputId": "21d38f4e-ee2c-4f03-dcd2-666ef659e73a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2C Accuracy: 0.9357\n",
      "A2C Precision (Class 1 - Fraud): 0.7542\n",
      "A2C Recall    (Class 1 - Fraud): 0.9082\n",
      "A2C F1 Score  (Class 1 - Fraud): 0.8241\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy_ac  = accuracy_score(true_labels_ac, predicted_labels_ac)\n",
    "precision_ac = precision_score(true_labels_ac, predicted_labels_ac, zero_division=0)\n",
    "recall_ac    = recall_score(true_labels_ac, predicted_labels_ac, zero_division=0)\n",
    "f1_ac        = f1_score(true_labels_ac, predicted_labels_ac, zero_division=0)\n",
    "\n",
    "print(f\"A2C Accuracy: {accuracy_ac:.4f}\")\n",
    "print(f\"A2C Precision (Class 1 - Fraud): {precision_ac:.4f}\")\n",
    "print(f\"A2C Recall    (Class 1 - Fraud): {recall_ac:.4f}\")\n",
    "print(f\"A2C F1 Score  (Class 1 - Fraud): {f1_ac:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gxI7fSElwsip",
    "outputId": "8328fc5c-d60d-458b-e796-3274549d37fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2C model saved to .models/a2c_fraud_model.zip\n"
     ]
    }
   ],
   "source": [
    "# 1. Choose a save path (no need to add .zipSB3 will append it for you)\n",
    "save_path = \"models/a2c_fraud_model\"\n",
    "\n",
    "# 2. Save your A2C model\n",
    "ac_model.save(save_path)\n",
    "\n",
    "# 3. Confirm it on screen\n",
    "print(f\"A2C model saved to {save_path}.zip\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
